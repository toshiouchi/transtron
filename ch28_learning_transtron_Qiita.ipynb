{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import japanize_matplotlib\n",
    "\n",
    "# warning表示off\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# デフォルトフォントサイズ変更\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# デフォルトグラフサイズ変更\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "# デフォルトで方眼表示ON\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# numpyの表示桁数設定\n",
    "np.set_printoptions(suppress=True, precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ttslearn.env import is_colab\n",
    "from os.path import exists\n",
    "\n",
    "# recipeのディレクトリに移動\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith(\"notebooks\"):\n",
    "    os.chdir(\"../recipes/transtron/\")\n",
    "elif is_colab():\n",
    "    os.chdir(\"recipes/transtron/\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb3714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def forward(self, x):\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight.to(x.dtype),\n",
    "            None if self.bias is None else self.bias.to(x.dtype),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b097e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        xa = None,\n",
    "        mask = None\n",
    "    ):\n",
    "        #print( \" size of x:{}\".format( x.size() ))\n",
    "        q = self.query(x)\n",
    "\n",
    "        if xa is None:\n",
    "            # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n",
    "            # otherwise, perform key/value projections for self- or cross-attention as usual.\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            k = self.key( xa )\n",
    "            v = self.value( xa )\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(\n",
    "        self, q, k, v, mask = None\n",
    "    ):\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * scale\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 3, 1) * scale\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk = q @ k\n",
    "        if mask is not None:\n",
    "            qk = qk + mask[:n_ctx, :n_ctx]\n",
    "        qk = qk.float()\n",
    "\n",
    "        w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526007f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_self_attn = MultiHeadAttention( 512, 2 )\n",
    "\n",
    "input_self_attn = torch.ones( (2,130,512))\n",
    "input_self_attn_mask = torch.ones( (512,512))\n",
    "\n",
    "a = tmp_self_attn( input_self_attn, mask = input_self_attn_mask )\n",
    "print( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1634c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_cross_attn = MultiHeadAttention( 512, 2)\n",
    "\n",
    "input_corss_attn_a = torch.ones( (2,130,512)) \n",
    "input_cross_attn_b = torch.ones( (2,1300,512)) \n",
    "input_cross_attn_mask = None\n",
    "\n",
    "b = tmp_cross_attn( input_corss_attn_a, input_cross_attn_b )\n",
    "\n",
    "print( b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8142463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attn = MultiHeadAttention(n_state, n_head ) if cross_attention else None\n",
    "        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(nn.Linear(n_state, n_mlp), nn.ReLU(), nn.Linear(n_mlp, n_state))\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        xa,\n",
    "        mask = None\n",
    "    ):\n",
    "        #x = x + self.attn(self.attn_ln(x), self.attn_ln(x), self.attn_ln(x), attn_mask=mask)[0]\n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask)[0]\n",
    "        if self.cross_attn:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, mask = None)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = ResidualAttentionBlock( n_state = 512, n_head = 2, cross_attention = False)\n",
    "#encoder_layer.eval()\n",
    "\n",
    "x = torch.ones( (2, 130, 512 ))\n",
    "mask = torch.ones( ( 130, 130 ))\n",
    "\n",
    "a = encoder_layer( x, x, mask = None )\n",
    "\n",
    "print( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b893736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_vocab=51,\n",
    "        embed_dim=512,\n",
    "        conv_layers=3,\n",
    "        conv_channels=512,\n",
    "        conv_kernel_size=5,\n",
    "        num_enc_layers = 3,\n",
    "        num_heads = 2,\n",
    "        enc_dropout_rate = 0.1,\n",
    "        conv_dropout_rate = 0.1,\n",
    "        input_maxlen = 300,\n",
    "        ffn_dim = 1024\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        # 文字の埋め込み表現\n",
    "        self.embed = nn.Embedding(num_vocab, embed_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(input_maxlen, embed_dim)\n",
    "        # 1 次元畳み込みの重ね合わせ：局所的な時間依存関係のモデル化\n",
    "        convs = nn.ModuleList()\n",
    "        for layer in range(conv_layers):\n",
    "            in_channels = embed_dim if layer == 0 else embed_dim\n",
    "            convs += [\n",
    "                nn.Conv1d(\n",
    "                    in_channels,\n",
    "                    conv_channels,\n",
    "                    conv_kernel_size,\n",
    "                    padding=(conv_kernel_size - 1) // 2,\n",
    "                    bias=False,  # この bias は不要です\n",
    "                ),\n",
    "                nn.BatchNorm1d(conv_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(conv_dropout_rate),\n",
    "            ]\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(embed_dim, num_heads) for _ in range(num_enc_layers)]\n",
    "        )\n",
    "        self.input_maxlen = input_maxlen\n",
    "        hidden_dim = embed_dim\n",
    "                \n",
    "        #self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "        #    [ResidualAttentionBlock(hidden_dim, num_heads) for _ in range(n_layer)]\n",
    "        #)\n",
    "        self.dropout = nn.Dropout(p=enc_dropout_rate)\n",
    "        self.num_enc_layers = num_enc_layers\n",
    "        \n",
    "    def forward(self, x, in_lens ):\n",
    "        emb = self.embed(x)\n",
    "        # 1 次元畳み込みと embedding では、入力のサイズ が異なるので注意\n",
    "        out = self.convs(emb.transpose(1, 2)).transpose(1, 2)\n",
    "        #print( \"encoder out:{}\".format( out ))\n",
    "        maxlen = out.size()[1]\n",
    "        #print( \"size of out:{}\".format( out.size()))\n",
    "        positions = torch.range(start=0, end=self.input_maxlen - 1, step=1).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:maxlen,:]\n",
    "        #print( \"size of positions:{}\".format( positions.size()))\n",
    "        x = out + positions\n",
    "        #print( \"0 encoder x:{}\".format( x ))\n",
    "        x = self.dropout( x )\n",
    "        #print( \"1 encoder x:{}\".format( x ))\n",
    "        #for i in range(self.num_enc_layers):\n",
    "        #    x = self.enc_layers[i](x )\n",
    "        #print( \"2 x:{}\".format( x ))\n",
    "        for block in self.blocks:\n",
    "            x = block(x, x, mask = None)\n",
    "        \n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40de95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_encoder = Encoder(\n",
    "        num_vocab=51,\n",
    "        embed_dim=512,\n",
    "        conv_layers=3,\n",
    "        conv_channels=512,\n",
    "        conv_kernel_size=5,\n",
    "        num_enc_layers = 3,\n",
    "        num_heads = 2,\n",
    "        enc_dropout_rate = 0.1,\n",
    "        conv_dropout_rate = 0.1,\n",
    "        input_maxlen = 300,\n",
    "        ffn_dim = 1024\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_encoder.eval()\n",
    "a = torch.ones( (2, 130), dtype=torch.long)\n",
    "\n",
    "in_lens = []\n",
    "for i in a:\n",
    "    in_lens.append( len(a)) \n",
    "\n",
    "\n",
    "b = tmp_encoder( a, in_lens )\n",
    "print( \" size of b:{}\".format( b.size()))\n",
    "print( b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ebd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, layers=2, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        prenet = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            prenet += [\n",
    "                nn.Linear(in_dim if layer == 0 else hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout) # added by Toshio Uchiyama\n",
    "            ]\n",
    "        self.prenet = nn.Sequential(*prenet)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.prenet:\n",
    "            # 学習時、推論時の両方で Dropout を適用します\n",
    "            #x = F.dropout(layer(x), self.dropout, training=True)\n",
    "            x = layer(x) # original is above\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3be182",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = ResidualAttentionBlock( n_state = 512, n_head = 2, cross_attention = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder_layer.eval()\n",
    "\n",
    "x = torch.ones( (2, 1300, 512 ))\n",
    "mask = torch.ones( ( 1300, 1300 ))\n",
    "\n",
    "a = decoder_layer( x, x, mask = mask )\n",
    "\n",
    "print( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de967f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        decoder_hidden_dim=512,\n",
    "        out_dim=80,\n",
    "        layers=4,\n",
    "        prenet_layers=2,\n",
    "        prenet_hidden_dim=512,\n",
    "        prenet_dropout=0.5,\n",
    "        ffn_dim=1024,\n",
    "        dropout_rate = 0.1,\n",
    "        dec_input_maxlen=3000,\n",
    "        num_heads = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Prenet\n",
    "        self.prenet = Prenet(out_dim, prenet_layers, prenet_hidden_dim, prenet_dropout)\n",
    "        #self.prenet = nn.Linear( out_dim, prenet_hidden_dim )\n",
    "        #self.prenet.eval()\n",
    "\n",
    "        #  DecoderLayer\n",
    "        #self.dec_layers = [DecoderLayer(decoder_hidden_dim, num_heads, ffn_dim, dropout_rate) \n",
    "        #               for _ in range(layers)]\n",
    "        \n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(decoder_hidden_dim, num_heads, cross_attention=True) for _ in range(layers)]\n",
    "        )\n",
    "        #self.blocks.eval()\n",
    "        \n",
    "        #self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.pos_emb = nn.Embedding(dec_input_maxlen, decoder_hidden_dim)\n",
    "        #self.pos_emb.eval()\n",
    "        \n",
    " \n",
    "        # 出力への projection 層\n",
    "        proj_in_dim = decoder_hidden_dim\n",
    "        #print( \"proj_in_dim:{}\".format( proj_in_dim ))\n",
    "        #print( \"out_dim:{}\".format( out_dim ))\n",
    "        self.feat_out = nn.Linear(proj_in_dim, out_dim, bias=False)\n",
    "        #self.feat_out.eval()\n",
    "        self.prob_out = nn.Linear(proj_in_dim, 1)\n",
    "        #self.prob_out.eval()\n",
    "        \n",
    "        self.dec_input_maxlen = dec_input_maxlen\n",
    "        self.layers = layers\n",
    "        hidden_dim = decoder_hidden_dim\n",
    "\n",
    "\n",
    "    def forward(self, encoder_outs, in_lens, decoder_targets=None):\n",
    "\n",
    "        # Pre-Net\n",
    "        #prenet_out = self.prenet(prev_out)\n",
    "        #print( \" size of decoder_targets:{}\".format( decoder_targets.size()))\n",
    "        #print( \"encoder_outs:{}\".format(encoder_outs) )\n",
    "        #print( \"decoder_targets:{}\".format( decoder_targets))\n",
    "        prenet_out = self.prenet(decoder_targets)\n",
    "        #print( \"prenet_out:{}\".format( prenet_out))\n",
    "        maxlen = prenet_out.size()[1]\n",
    "        #print( \"size of prenet_out:{}\".format( prenet_out.size()))\n",
    "        positions = torch.range(start=0, end=self.dec_input_maxlen - 1, step=1).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:maxlen,:]\n",
    "        #print( \"positions:{}\".format( positions))\n",
    "        #print( \"size of positions:{}\".format( positions.size()))\n",
    "        x = prenet_out + positions\n",
    "        #print( \"0 x:{}\".format( x ))\n",
    "        \n",
    "        attention_weights = {}\n",
    "        \n",
    "        # DecoderLayer\n",
    "        #for i in range(self.layers):\n",
    "        #    #print( \"0 size of x:{}\".format( x.size()))\n",
    "        #    T = x.size()[1]\n",
    "        #    #T = 1\n",
    "        #    look_ahead_mask = torch.empty(T, T).fill_(-np.inf).triu_(1)\n",
    "        #    #look_ahead_mask = torch.triu(torch.full((T, T), float('-inf')), diagonal=1).type(torch.bool)\n",
    "        #    x, block1, block2  = self.dec_layers[i](encoder_outs, x, look_ahead_mask)\n",
    "        #    attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "        #    attention_weights['decoder_layer{}_block2'.format(i+1)] = block2   \n",
    "        #    #print( \"1 size of x:{}\".format( x.size()))\n",
    "\n",
    "        for block in self.blocks:\n",
    "            T = x.size()[1]\n",
    "            #T = 1\n",
    "            look_ahead_mask = torch.empty(T, T).fill_(-np.inf).triu_(1)\n",
    "            x = block(x, encoder_outs, mask=look_ahead_mask)            \n",
    "            \n",
    "        #print( \"size of x:{}\".format( x.size()))\n",
    "        outs = self.feat_out(x)\n",
    "        #print( \"outs:{}\".format(outs))\n",
    "        outs = torch.permute(outs, (0, 2, 1))\n",
    "        logits = torch.squeeze( self.prob_out(x), axis=2 )            \n",
    "        \n",
    "        return outs, logits, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_decoder = Decoder(\n",
    "        decoder_hidden_dim=512,\n",
    "        out_dim=80,\n",
    "        layers=2,\n",
    "        prenet_layers=2,\n",
    "        prenet_hidden_dim=512,\n",
    "        prenet_dropout=0.5,\n",
    "        ffn_dim=1024,\n",
    "        dropout_rate = 0.1,\n",
    "        dec_input_maxlen=3000,\n",
    "        num_heads = 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_decoder.eval()\n",
    "\n",
    "a = torch.ones( (2, 130, 512), dtype=torch.float)\n",
    "\n",
    "in_lens = []\n",
    "for i in a:\n",
    "    in_lens.append( len(a)) \n",
    "\n",
    "b = torch.ones( (2, 1300, 80 ), dtype=torch.float )\n",
    "\n",
    "c, d, e = tmp_decoder( a, in_lens, b )\n",
    "\n",
    "print( \"size of c:{}\".format( c.size()))\n",
    "print( c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Postnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim=80,\n",
    "        layers=5,\n",
    "        channels=512,\n",
    "        kernel_size=5,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        postnet = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            in_channels = in_dim if layer == 0 else channels\n",
    "            out_channels = in_dim if layer == layers - 1 else channels\n",
    "            postnet += [\n",
    "                nn.Conv1d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=(kernel_size - 1) // 2,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            ]\n",
    "            if layer != layers - 1:\n",
    "                postnet += [nn.Tanh()]\n",
    "            postnet += [nn.Dropout(dropout)]\n",
    "        self.postnet = nn.Sequential(*postnet)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return self.postnet(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transtron(nn.Module):\n",
    "    def __init__(self,\n",
    "            num_vocab=52,\n",
    "            embed_dim=512,\n",
    "            conv_layers=3,\n",
    "            conv_channels=512,\n",
    "            conv_kernel_size=5,\n",
    "            num_enc_layers = 4,\n",
    "            enc_num_heads = 2,\n",
    "            enc_dropout_rate = 0.1,\n",
    "            conv_dropout_rate = 0.1,\n",
    "            enc_input_maxlen = 300,\n",
    "            enc_ffn_dim = 1024,              \n",
    "            decoder_hidden_dim=512,\n",
    "            out_dim=80,\n",
    "            num_dec_layers=4,\n",
    "            prenet_layers=2,\n",
    "            prenet_hidden_dim=512,\n",
    "            prenet_dropout=0.5,\n",
    "            dec_ffn_dim=1024,\n",
    "            dec_dropout_rate = 0.1,\n",
    "            dec_input_maxlen=3000,\n",
    "            dec_num_heads = 2,                \n",
    "            postnet_in_dim=80,\n",
    "            postnet_layers=5,\n",
    "            postnet_channels=512,\n",
    "            postnet_kernel_size=5,\n",
    "            postnet_dropout=0.5\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            num_vocab,\n",
    "            embed_dim,\n",
    "            conv_layers,\n",
    "            conv_channels,\n",
    "            conv_kernel_size,\n",
    "            num_enc_layers,\n",
    "            enc_num_heads,\n",
    "            enc_dropout_rate,\n",
    "            conv_dropout_rate,\n",
    "            enc_input_maxlen,\n",
    "            enc_ffn_dim \n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            decoder_hidden_dim,\n",
    "            out_dim,\n",
    "            num_dec_layers,\n",
    "            prenet_layers,\n",
    "            prenet_hidden_dim,\n",
    "            prenet_dropout,\n",
    "            dec_ffn_dim,\n",
    "            dec_dropout_rate,\n",
    "            dec_input_maxlen,\n",
    "            dec_num_heads       \n",
    "        )\n",
    "        self.postnet = Postnet(\n",
    "            postnet_in_dim,\n",
    "            postnet_layers,\n",
    "            postnet_channels,\n",
    "            postnet_kernel_size,\n",
    "            postnet_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, in_lens, decoder_targets):\n",
    "        # エンコーダによるテキストに潜在する表現の獲得\n",
    "        encoder_outs = self.encoder(seq, in_lens)\n",
    "\n",
    "        # デコーダによるメルスペクトログラム、stop token の予測\n",
    "        outs, logits, att_ws = self.decoder(encoder_outs, in_lens, decoder_targets)\n",
    "\n",
    "        # Post-Net によるメルスペクトログラムの残差の予測\n",
    "        outs_fine = outs + self.postnet(outs)\n",
    "\n",
    "        # (B, C, T) -> (B, T, C)\n",
    "        outs = outs.transpose(2, 1)\n",
    "        outs_fine = outs_fine.transpose(2, 1)\n",
    "\n",
    "        return outs, outs_fine, logits, att_ws\n",
    "    '''\n",
    "    def inference(self, seq):\n",
    "        seq = seq.unsqueeze(0) if len(seq.shape) == 1 else seq\n",
    "        in_lens = torch.tensor([seq.shape[-1]], dtype=torch.long, device=seq.device)\n",
    "\n",
    "        return self.forward(seq, in_lens, None)\n",
    "    def inference(self, in_feats ):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        in_feats = torch.unsqueeze( in_feats, axis = 0 )\n",
    "        bs = in_feats.size()[0]\n",
    "        in_lens = []\n",
    "        for feats in ( in_feats):\n",
    "            in_lens.append( len( feats ))\n",
    "        # エンコーダによるテキストに潜在する表現の獲得\n",
    "        encoder_outs = model.encoder(in_feats, in_lens)\n",
    "        decoder_targets_maxlen = in_lens[0] * 10\n",
    "        #dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        decoder_targets = encoder_outs.new_zeros((encoder_outs.size()[0], 1, 80))\n",
    "        #decoder_targets = None\n",
    "        #dec_logits = []\n",
    "        for i in range(decoder_targets_maxlen ):\n",
    "            print( \"i:{}\".format( i ))\n",
    "            # デコーダによるメルスペクトログラム、stop token の予測\n",
    "            outs, logits, att_ws = model.decoder(encoder_outs, in_lens, decoder_targets)\n",
    "            print( \"torch.sigmoid(logits[0, -1]):{}\".format(torch.sigmoid(logits[0, -1])))\n",
    "            if i > 40 and torch.sigmoid(logits[0, -1]) >= 0.5:\n",
    "                break\n",
    "            #print( \"0 size of outs:{}\".format( outs.size() ))\n",
    "            outs = torch.permute(outs, (0, 2, 1))\n",
    "            outs2 = torch.unsqueeze( outs[:,-1,:], axis = 1 )\n",
    "            #print( \"size of outs2:{}\".format( outs2.size()))\n",
    "            #print( \"1 size of outs:{}\".format( outs.size() ))\n",
    "            #print( \"1 size of decoder_targets:{}\".format( decoder_targets.size()))\n",
    "            decoder_targets = torch.cat( (decoder_targets, outs2), axis = 1 )\n",
    "            #print( \"2 size of decoder_targets:{}\".format( decoder_targets.size()))\n",
    "            #logits = self.classifier(dec_out)\n",
    "            #logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            #last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
    "            #decoder_targets = torch.concat([decoder_targets, outs], axis=-1)\n",
    "        # Post-Net によるメルスペクトログラムの残差の予測\n",
    "        outs = torch.permute(outs, (0, 2, 1))\n",
    "        outs_fine = outs + model.postnet(outs)\n",
    "\n",
    "        # (B, C, T) -> (B, T, C)\n",
    "        outs = outs.transpose(2, 1)\n",
    "        outs_fine = outs_fine.transpose(2, 1)\n",
    "    \n",
    "        #print( \"size of outs_fine:{}\".format( outs_fine.size() ))\n",
    "    \n",
    "        return outs[0], outs_fine[0], logits[0], att_ws  \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865cc406",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transtron(\n",
    "    num_vocab=52,\n",
    "    embed_dim=512,\n",
    "    conv_layers=3,\n",
    "    conv_channels=512,\n",
    "    conv_kernel_size=5,\n",
    "    num_enc_layers = 4,\n",
    "    enc_num_heads = 2,\n",
    "    enc_dropout_rate = 0.1,\n",
    "    conv_dropout_rate = 0.1,\n",
    "    enc_input_maxlen = 300,\n",
    "    enc_ffn_dim = 2048,          \n",
    "    decoder_hidden_dim=512,\n",
    "    out_dim=80,\n",
    "    num_dec_layers=4,\n",
    "    prenet_layers=2,\n",
    "    prenet_hidden_dim=512,\n",
    "    prenet_dropout=0.5,\n",
    "    dec_ffn_dim=2048,\n",
    "    dec_dropout_rate = 0.1,\n",
    "    dec_input_maxlen=3000,\n",
    "    dec_num_heads = 2,                \n",
    "    postnet_in_dim=80,\n",
    "    postnet_layers=5,\n",
    "    postnet_channels=512,\n",
    "    postnet_kernel_size=5,\n",
    "    postnet_dropout=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcea981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb9177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習で必要な関数\n",
    "def ensure_divisible_by(feats, N):\n",
    "    if N == 1:\n",
    "        return feats\n",
    "    mod = len(feats) % N\n",
    "    if mod != 0:\n",
    "        feats = feats[: len(feats) - mod]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0121925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習で必要な関数\n",
    "from ttslearn.util import pad_1d, pad_2d\n",
    "\n",
    "def collate_fn_transtron(batch):\n",
    "    xs = [x[0] for x in batch]\n",
    "    ys = [ensure_divisible_by(x[1], 1) for x in batch]\n",
    "    in_lens = [len(x) for x in xs]\n",
    "    out_lens = [len(y) for y in ys]\n",
    "    in_max_len = max(in_lens)\n",
    "    out_max_len = max(out_lens)\n",
    "    x_batch = torch.stack([torch.from_numpy(pad_1d(x, in_max_len)) for x in xs])\n",
    "    y_batch = torch.stack([torch.from_numpy(pad_2d(y, out_max_len)) for y in ys])\n",
    "    in_lens = torch.tensor(in_lens, dtype=torch.long)\n",
    "    out_lens = torch.tensor(out_lens, dtype=torch.long)\n",
    "    stop_flags = torch.zeros(y_batch.shape[0], y_batch.shape[1])\n",
    "    for idx, out_len in enumerate(out_lens):\n",
    "        stop_flags[idx, out_len - 1 :] = 1.0\n",
    "    return x_batch, in_lens, y_batch, out_lens, stop_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653fa8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習で必要なミニバッチデータ\n",
    "from pathlib import Path\n",
    "from ttslearn.train_util import Dataset\n",
    "from functools import partial\n",
    "\n",
    "in_paths_dev = sorted(Path(\"./dump/jsut_sr16000/norm/dev/in_tacotron/\").glob(\"*.npy\"))\n",
    "in_paths = sorted(Path(\"./dump/jsut_sr16000/norm/train/in_tacotron/\").glob(\"*.npy\"))\n",
    "#in_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/in_tacotron/\").glob(\"*.npy\"))\n",
    "#print( \"in_paths:{}\".format( in_paths ))\n",
    "out_paths_dev = sorted(Path(\"./dump/jsut_sr16000/norm/dev/out_tacotron/\").glob(\"*.npy\"))\n",
    "out_paths = sorted(Path(\"./dump/jsut_sr16000/norm/train/out_tacotron/\").glob(\"*.npy\"))\n",
    "#out_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/out_tacotron/\").glob(\"*.npy\"))\n",
    "\n",
    "\n",
    "dataset = Dataset(in_paths, out_paths)\n",
    "dataset_dev = Dataset(in_paths_dev, out_paths_dev)\n",
    "#print( \" len of dataset:{}\".format( len( dataset )))\n",
    "collate_fn = partial(collate_fn_transtron)\n",
    "#data_loader = torch.utils.data.DataLoader(dataset, batch_size=8, collate_fn=collate_fn, num_workers=0)\n",
    "#data_loader_dev = torch.utils.data.DataLoader(dataset_dev, batch_size=8, collate_fn=collate_fn, num_workers=0)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, collate_fn=collate_fn, num_workers=0)\n",
    "data_loader_dev = torch.utils.data.DataLoader(dataset_dev, batch_size=16, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "\n",
    "in_feats, in_lens, out_feats, out_lens, stop_flags = next(iter(data_loader))\n",
    "print(\"入力特徴量のサイズ:\", tuple(in_feats.shape))\n",
    "print(\"出力特徴量のサイズ:\", tuple(out_feats.shape))\n",
    "print(\"stop flags のサイズ:\", tuple(stop_flags.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ac81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習前にミニバチデータの可視化（教師データ,out_feats)\n",
    "\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
    "cmap = get_cmap()\n",
    "init_plot_style()\n",
    "\n",
    "sr = 16000\n",
    "\n",
    "fig, ax = plt.subplots(len(out_feats), 1, figsize=(8,10), sharex=True, sharey=True)\n",
    "for n in range(len(in_feats)):\n",
    "    x = out_feats[n].data.numpy()\n",
    "    hop_length = int(sr * 0.0125)\n",
    "    mesh = librosa.display.specshow(x.T, sr=sr, x_axis=\"time\", y_axis=\"frames\", hop_length=hop_length, cmap=cmap, ax=ax[n])\n",
    "    fig.colorbar(mesh, ax=ax[n])\n",
    "    mesh.set_clim(-4, 4)\n",
    "    # あとで付け直すので、ここではラベルを削除します\n",
    "    ax[n].set_xlabel(\"\")\n",
    "    \n",
    "ax[-1].set_xlabel(\"Time [sec]\")\n",
    "for a in ax:\n",
    "    a.set_ylabel(\"Mel channel\")\n",
    "\n",
    "plt.tight_layout()\n",
    "savefig(\"fig/e2etts_impl_minibatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8928972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習の前準備\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "# lr は学習率を表します\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, eps=1e-9, amsgrad=True)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, eps=1e-9, amsgrad=True)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=0.0001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "\n",
    "\n",
    "# gamma は学習率の減衰係数を表します\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690778ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "\n",
    "gc.collect()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習\n",
    "\n",
    "from ttslearn.util import make_non_pad_mask\n",
    "from ttslearn.tacotron import Tacotron2TTS\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import Audio\n",
    "\n",
    "history = np.zeros((0, 7))\n",
    "history_dev = np.zeros((0, 6))\n",
    "\n",
    "num_epochs = 1000\n",
    "#num_epochs = 30\n",
    "it_train = 0\n",
    "it_dev = 0\n",
    "for epoch in range( num_epochs ):\n",
    "    \n",
    "    model.train()\n",
    "    total_decoder_out_loss = 0\n",
    "    total_postnet_out_loss = 0\n",
    "    total_stop_token_loss = 0\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    # DataLoader を用いたミニバッチの作成: ミニバッチ毎に処理する\n",
    "    phar = tqdm( range( len(data_loader) ), desc='train' )\n",
    "    Iter_train = iter(data_loader)\n",
    "    for i in phar:\n",
    "    #for in_feats, in_lens, out_feats, out_lens, stop_flags in tqdm(data_loader):\n",
    "        in_feats, in_lens, out_feats, out_lens, stop_flags = next(Iter_train)\n",
    "        in_lens, indices = torch.sort(in_lens, dim=0, descending=True)\n",
    "        in_feats, out_feats, out_lens = in_feats[indices], out_feats[indices], out_lens[indices]\n",
    "        out_feats0 = torch.zeros_like( out_feats )\n",
    "        out_feats0[:,1:,:] = out_feats[:,:-1,:]\n",
    "    \n",
    "        #count += len( in_feats )\n",
    "        count += 1\n",
    "    \n",
    "        # 順伝搬の計算\n",
    "        #print( \"size of in_feats:{}\".format( in_feats.size()))\n",
    "        #print( \"size of in_lens:{}\".format( in_lens.size()))\n",
    "        #print( \"in_lens:{}\".format( in_lens ))\n",
    "        #print( \"size of out_feats:{}\".format( out_feats.size ))\n",
    "        #out_feats2 = out_feats[:,:-1,:]\n",
    "    \n",
    "        #outs, outs_fine, logits, _ = model(in_feats, in_lens, out_feats)\n",
    "        outs, outs_fine, logits, _ = model(in_feats, in_lens, out_feats0)\n",
    "        #print( \"size of out_feats:{}\".format( out_feats.size()))\n",
    "        #out_feats2 = torch.zeros_like( out_feats )\n",
    "        #out_feats2[:,:-1,:] = out_feats[:,1:,:]\n",
    "        #print( \"size of out_feats2:{}\".format( out_feats2.size()))\n",
    "        #stop_flags2 = torch.ones_like( stop_flags )\n",
    "        #stop_flags2[:,:-1] = stop_flags[:,1:] \n",
    "        \n",
    "        # ゼロパディグした部分を損失関数のの計算から除外するためにマスクを適用します\n",
    "        # Mask (B x T x 1)\n",
    "        mask = make_non_pad_mask(out_lens).unsqueeze(-1)\n",
    "        #print( out_feats2.size())\n",
    "        out_feats = out_feats.masked_select(mask)\n",
    "        #out_feats2 = out_feats2.masked_select(mask)\n",
    "        outs = outs.masked_select(mask)\n",
    "        outs_fine = outs_fine.masked_select(mask)\n",
    "        #print( \"size of stop_flags:{}\".format( stop_flags.size()))\n",
    "        #print( \"stop_flags[0][-1]:{}\".format( stop_flags[0][-1]))\n",
    "        stop_flags = stop_flags.masked_select(mask.squeeze(-1))\n",
    "        #stop_flags2 = stop_flags2.masked_select(mask.squeeze(-1))\n",
    "        logits = logits.masked_select(mask.squeeze(-1))\n",
    "        #print( out_feats.size())\n",
    "        \n",
    "        # 損失の計算\n",
    "        #decoder_out_loss = nn.MSELoss(reduction='mean')(outs, out_feats2)\n",
    "        #decoder_out_loss = nn.MSELoss()(outs, out_feats2)\n",
    "        #postnet_out_loss = nn.MSELoss(reduction='mean')(outs_fine, out_feats2)\n",
    "        #postnet_out_loss = nn.MSELoss()(outs_fine, out_feats2)\n",
    "        decoder_out_loss = nn.MSELoss()(outs, out_feats)\n",
    "        postnet_out_loss = nn.MSELoss()(outs_fine, out_feats) \n",
    "        #print( \"logits\", logits )\n",
    "        #print( \"stop_flags\", stop_flags)\n",
    "        stop_token_loss = nn.BCEWithLogitsLoss()(logits, stop_flags)\n",
    "        #stop_token_loss = nn.BCEWithLogitsLoss(reduction='mean')(logits, stop_flags2)\n",
    "        #stop_token_loss = nn.BCEWithLogitsLoss()(logits, stop_flags2)\n",
    "        \n",
    "        # 損失の合計\n",
    "        loss = decoder_out_loss + postnet_out_loss + stop_token_loss\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        #print( \"loss:{}\".format(total_loss))\n",
    "        total_decoder_out_loss += decoder_out_loss.item()\n",
    "        #print( \"decoder_out_loss:{}\".format(total_decoder_out_loss))\n",
    "        total_postnet_out_loss += postnet_out_loss.item()\n",
    "        #print( \"postnet_out_loss:{}\".format(total_postnet_out_loss))\n",
    "        total_stop_token_loss += stop_token_loss.item()\n",
    "        #print( \"stop_token_loss:{}\".format(total_stop_token_loss))\n",
    "\n",
    "        \n",
    "        # 損失の値を出力\n",
    "        it_train += 1\n",
    "        # optimizer に蓄積された勾配をリセット\n",
    "        optimizer.zero_grad()\n",
    "        # 誤差の逆伝播\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        #a = nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2)\n",
    "        #a = nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
    "        #print( \"a:{}\".format(a))\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0, norm_type=2)\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5, norm_type=2)\n",
    "        #nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)\n",
    "        # パラメータの更新\n",
    "        optimizer.step()\n",
    "        # 学習率スケジューラの更新\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / count\n",
    "        \n",
    "        #プログラスバーに cer 表示\n",
    "        phar.set_postfix( loss = avg_loss )   \n",
    "        \n",
    "    avg_loss = total_loss / count\n",
    "    avg_decoder_out_loss = total_decoder_out_loss / count\n",
    "    avg_postnet_out_loss = total_postnet_out_loss / count\n",
    "    avg_stop_token_loss = total_stop_token_loss / count    \n",
    "\n",
    "    print(f\"epoch: {epoch+1:3d}, train it: {it_train:6d}, decoder_out: {avg_decoder_out_loss :.5f}, postnet_out: {avg_postnet_out_loss :.5f}, stop_token: {avg_stop_token_loss :.5f}, loss: {avg_loss :.5f}\")\n",
    "    item = np.array([epoch+1, it_train, avg_decoder_out_loss , avg_postnet_out_loss , avg_stop_token_loss , avg_loss ,  current_lr ])\n",
    "    history = np.vstack((history, item))\n",
    "    \n",
    "    model.eval()\n",
    "    total_dev_decoder_out_loss = 0\n",
    "    total_dev_postnet_out_loss = 0\n",
    "    total_dev_stop_token_loss = 0\n",
    "    total_dev_loss = 0\n",
    "    count = 0\n",
    "    # DataLoader を用いたミニバッチの作成: ミニバッチ毎に処理する\n",
    "    phar = tqdm( range( len(data_loader_dev) ), desc='dev' )\n",
    "    Iter_dev = iter(data_loader_dev)\n",
    "    for i in phar:\n",
    "    #for in_feats, in_lens, out_feats, out_lens, stop_flags in tqdm(data_loader_dev):\n",
    "        in_feats, in_lens, out_feats, out_lens, stop_flags = next(Iter_dev)\n",
    "        in_lens, indices = torch.sort(in_lens, dim=0, descending=True)\n",
    "        in_feats, out_feats, out_lens = in_feats[indices], out_feats[indices], out_lens[indices]\n",
    "        out_feats0 = torch.zeros_like( out_feats )\n",
    "        out_feats0[:,1:,:] = out_feats[:,:-1,:]\n",
    "        \n",
    "        #count += len( in_feats )\n",
    "        count += 1\n",
    "   \n",
    "        #outs, outs_fine, logits, _ = model(in_feats, in_lens, out_feats)\n",
    "        outs, outs_fine, logits, _ = model(in_feats, in_lens, out_feats0)\n",
    "        #out_feats2 = torch.zeros_like( out_feats )\n",
    "        #out_feats2[:,:-1,:] = out_feats[:,1:,:]    \n",
    "        #stop_flags2 = torch.ones_like( stop_flags )\n",
    "        #stop_flags2[:,:-1] = stop_flags[:,1:] \n",
    "        \n",
    "        # ゼロパディグした部分を損失関数のの計算から除外するためにマスクを適用します\n",
    "        # Mask (B x T x 1)\n",
    "        mask = make_non_pad_mask(out_lens).unsqueeze(-1)\n",
    "        #out_feats2 = out_feats2.masked_select(mask)\n",
    "        out_feats = out_feats.masked_select(mask)\n",
    "        outs = outs.masked_select(mask)\n",
    "        outs_fine = outs_fine.masked_select(mask)\n",
    "        stop_flags = stop_flags.masked_select(mask.squeeze(-1))\n",
    "        #stop_flags2 = stop_flags2.masked_select(mask.squeeze(-1))\n",
    "        logits = logits.masked_select(mask.squeeze(-1))\n",
    "        \n",
    "        # 損失の計算\n",
    "        #print( \" size of outs:{}\".format( outs.size()))\n",
    "        #print( \" size of out_feats2:{}\".format( out_feats2.size()))\n",
    "        #dev_decoder_out_loss = nn.MSELoss(reduction='mean')(outs, out_feats2)\n",
    "        #dev_decoder_out_loss = nn.MSELoss()(outs, out_feats2)\n",
    "        #dev_postnet_out_loss = nn.MSELoss(reduction='mean')(outs_fine, out_feats2)\n",
    "        #dev_postnet_out_loss = nn.MSELoss()(outs_fine, out_feats2)\n",
    "        dev_decoder_out_loss = nn.MSELoss()(outs, out_feats)\n",
    "        dev_postnet_out_loss = nn.MSELoss()(outs_fine, out_feats) \n",
    "        dev_stop_token_loss = nn.BCEWithLogitsLoss()(logits, stop_flags)\n",
    "        #dev_stop_token_loss = nn.BCEWithLogitsLoss(reduction='mean')(logits, stop_flags2)\n",
    "        #dev_stop_token_loss = nn.BCEWithLogitsLoss()(logits, stop_flags2)\n",
    "        \n",
    "        # 損失の合計\n",
    "        dev_loss = dev_decoder_out_loss + dev_postnet_out_loss + dev_stop_token_loss\n",
    "        \n",
    "        total_dev_loss += dev_loss.item()\n",
    "        total_dev_decoder_out_loss += dev_decoder_out_loss.item()\n",
    "        total_dev_postnet_out_loss += dev_postnet_out_loss.item()\n",
    "        total_dev_stop_token_loss += dev_stop_token_loss.item()\n",
    "        \n",
    "        avg_dev_loss = total_dev_loss / count\n",
    "        \n",
    "        #プログラスバーに cer 表示\n",
    "        phar.set_postfix( dev_loss = avg_dev_loss ) \n",
    "\n",
    "        # 損失の値を出力\n",
    "        it_dev += 1\n",
    "        \n",
    "    avg_dev_loss = total_dev_loss / count\n",
    "    avg_dev_decoder_out_loss = total_dev_decoder_out_loss / count\n",
    "    avg_dev_postnet_out_loss = total_dev_postnet_out_loss / count\n",
    "    avg_dev_stop_token_loss = total_dev_stop_token_loss / count    \n",
    "\n",
    "    print(f\"epoch: {epoch+1:3d}, dev it: {it_dev:6d}, decoder_out: {avg_dev_decoder_out_loss:.5f}, postnet_out: {avg_dev_postnet_out_loss:.5f}, stop_token: {avg_dev_stop_token_loss:.5f}, loss: {avg_dev_loss:.5f}\")\n",
    "    item = np.array([epoch+1, it_dev, avg_dev_decoder_out_loss , avg_dev_postnet_out_loss , avg_dev_stop_token_loss , avg_dev_loss ])\n",
    "    history_dev = np.vstack((history_dev, item))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11956bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルのセーブ\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "hist_df = pd.DataFrame(history)\n",
    "hist_df.to_csv('history_ch28.csv', header=False, index=False)\n",
    "hist_dev_df = pd.DataFrame(history_dev)\n",
    "hist_dev_df.to_csv('history_dev_ch28.csv', header=False, index=False)\n",
    "\n",
    "torch.save(model, 'transtron_weight28.pth')\n",
    "torch.save(model.state_dict(), 'transtron_weight28_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a85ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "def inference( in_feats ):\n",
    "    \n",
    "    \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "    #print( in_feats )\n",
    "    in_feats = torch.unsqueeze( in_feats, axis = 0 )\n",
    "    bs = in_feats.size()[0]\n",
    "    in_lens = []\n",
    "    for feats in ( in_feats):\n",
    "        in_lens.append( len( feats ))\n",
    "    # エンコーダによるテキストに潜在する表現の獲得\n",
    "    #print( \"in_feats:{}\".format(in_feats) )\n",
    "    #print( \"in_lens:{}\".format( in_lens ))\n",
    "    encoder_outs = model.encoder(in_feats, in_lens)\n",
    "    #print( \"encoder_outs:{}\".format( encoder_outs ))\n",
    "    decoder_targets_maxlen = in_lens[0] * 10\n",
    "    #dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "    decoder_targets = encoder_outs.new_zeros((encoder_outs.size()[0], 1, 80))\n",
    "    #decoder_targets = None\n",
    "    #dec_logits = []\n",
    "    for i in range(decoder_targets_maxlen ):\n",
    "        print( \"i:{}\".format( i ))\n",
    "        # デコーダによるメルスペクトログラム、stop token の予測\n",
    "        outs, logits, att_ws = model.decoder(encoder_outs, in_lens, decoder_targets)\n",
    "        #print( \"torch.sigmoid(logits[0, -1]):{}\".format(torch.sigmoid(logits[0, -1])))\n",
    "        if i > 40 and torch.sigmoid(logits[0, -1]) >= 0.5:\n",
    "            break\n",
    "        #print( \"0 size of outs:{}\".format( outs.size() ))\n",
    "        outs = torch.permute(outs, (0, 2, 1))\n",
    "        outs2 = torch.unsqueeze( outs[:,-1,:], axis = 1 )\n",
    "        #print( \"size of outs2:{}\".format( outs2.size()))\n",
    "        #print( \"1 size of outs:{}\".format( outs.size() ))\n",
    "        #print( \"1 size of decoder_targets:{}\".format( decoder_targets.size()))\n",
    "        decoder_targets = torch.cat( (decoder_targets, outs2), axis = 1 )\n",
    "        #print( \"2 size of decoder_targets:{}\".format( decoder_targets.size()))\n",
    "        #logits = self.classifier(dec_out)\n",
    "        #logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "        #last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
    "        #decoder_targets = torch.concat([decoder_targets, outs], axis=-1)\n",
    "    # Post-Net によるメルスペクトログラムの残差の予測\n",
    "    outs = torch.permute(decoder_targets, (0, 2, 1))\n",
    "    outs_fine = outs + model.postnet(outs)\n",
    "\n",
    "    # (B, C, T) -> (B, T, C)\n",
    "    outs = outs.transpose(2, 1)\n",
    "    outs_fine = outs_fine.transpose(2, 1)\n",
    "    \n",
    "    #print( \"size of outs_fine:{}\".format( outs_fine.size() ))\n",
    "    \n",
    "    return outs[0], outs_fine[0], logits[0], att_ws  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
